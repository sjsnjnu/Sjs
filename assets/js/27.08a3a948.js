(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{436:function(t,a,s){"use strict";s.r(a);var e=s(2),v=Object(e.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"边缘计算中个性化深度神经网络的多任务联邦学习"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#边缘计算中个性化深度神经网络的多任务联邦学习"}},[t._v("#")]),t._v(" 边缘计算中个性化深度神经网络的多任务联邦学习")]),t._v(" "),a("h3",{attrs:{id:"abstract"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),a("p",[t._v("联邦学习（FL）是一种新兴的方法，用于在移动设备上协作训练深度神经网络（DNN），而无需私人用户数据离开设备。非独立同分布（non- IID）用户数据会影响FL算法的收敛速度。此外，关于FL的大多数现有工作都衡量全局模型的准确性，但在许多情况下，如用户内容推荐，提高单个用户模型的准确性（UA，Users model Accuracy）才是真正的目标。为了解决这些问题，我们提出了一种多任务FL（Multi- Task FL，MTFL）算法，该算法将非联邦批处理规范化（BN）层引入联邦DNN。MTFL允许用户根据自己的数据训练个性化模型，从而有利于UA和收敛速度。MTFL与常用的迭代FL优化算法（如联邦平均法FedAvg）兼容，我们从经验上证明，分布式形式的Adam优化（FedAvg-Adam）在MNIS T中用作优化策略时更能提高收敛速度。使用MNIST和CIFAR10的实验表明⌚️，MTFL能够显著减少达到目标UA所需的轮数，在使用现有的FL优化策略时最多减少5倍，在使用FedAvg Adam时进一步减少3倍。我们将MTFL与竞争的个性化FL算法进行了比较，表明它能够在所有考虑的场景中实现MNIST和CIFAR10的最佳UA。最后，我们在边缘计算实验台上使用Fedavg-Adam评估了MTFL，表明其收敛性和UA优势超过了其开销💰。")]),t._v(" "),a("h3",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),a("p",[t._v("​\t多址边缘计算MEC将云服务移动到网络边缘，通过内容缓存和计算卸载实现应用程序的低延迟和实时处理。")]),t._v(" "),a("p",[t._v("​\t用户机器学习的深度神经网络DNN因其广泛的潜在应用、易于部署和最先进的性能而日益流行。然而，在监督学习中训练DNN在计算上非常昂贵，需要大量的训练数据，特别是在DNN规模不断扩大的趋势下。在MEC中使用DNN通常涉及从移动/物联网设备收集数据，在云中☁️执行训练，然后在边缘部署模型。然而，对数据隐私的担忧意味着用户越来越不愿意上传它们潜在的敏感数据，这就提出了如何训练这些模型的问题。")]),t._v(" "),a("p",[t._v("​\t联邦学习为边缘的机器学习打开了新视野，在FL中，参与的客户机协作训练一个ML模型（通常是DNN），而不透露他们的私人数据。FedAvg的工作原理是将模型分发给客户端之前，在协调服务器上初始化模型。这些客户机在其本地数据集上执行一轮训练，并将其新模型推送到服务器。服务器将这些模型平均起来，然后将新的聚合模型发送给客户机进行下一轮培训。")]),t._v(" "),a("p",[t._v("​\tFL是一种非常有前途的分布式ML方法，适用于无法上传数据的情况，以保护客户的隐私。"),a("strong",[t._v("However，FL presents multiple unique challenges:")])]),t._v(" "),a("ol",[a("li",[t._v("Clients通常没有独立且同分布（IID）的训练数据。")]),t._v(" "),a("li",[t._v("FL通常使用全局模型精度的性能度量，然而许多情况下，客户的个人模型准确性才是真正的目标。Personalisted FL似乎是最好的解决办法。")]),t._v(" "),a("li",[t._v("由于客户端数据上的非IID性质，某些客户端的全局FL模型性能可能高于其他客户端。")])]),t._v(" "),a("p",[t._v("​\t本文通过提出MTFL来解决上述问题。该算法允许客户训练个性化DNN，既提高了本地模型的准确性，又有助于进一步增强客户隐私。与其他个性化FL算法相比，MTFL具有更低的个性化存储成本和更低的计算成本，在训练循环期间或个性化时不需要客户端进行额外的SGD步骤。🐟")]),t._v(" "),a("p",[t._v("​\t由于FL中的客户机收集到的数据通常是非IID的，因此将客户机视为在本地训练期间针对不同任务优化其模型。MTFL采用批量归一化（BN）层，这些层通常合并到DNN体系结构中，并对每个客户机保密。")]),t._v(" "),a("p",[t._v("​\t使用私有BN层有双层好处：对客户端本地数据进行个性化设置每个模型，并有助于保护数据隐私。与其他个性化FL相比，也具有存储成本优势：BN层通常包含DNN总参数的一小部分，并在FL轮之间只需要存储这些BN参数。")]),t._v(" "),a("p",[t._v("​\tMTFL在典型的迭代FL框架之上增加了个性化。FedAvg和其他流行算法就是这种迭代优化框架。大多数FL算法在客户机上使用普通随机梯度下降（SGD），然而，基于动量的优化策略（如Adam）有可能提高FL训练的收敛速度。我们表明，与Fed Avg相比使用Adam的分布式优化技术（FedAvg-Adam）在通信循环♻️中有更快的速度，并且在MTFL算法中非常有效。")]),t._v(" "),a("p",[t._v("​\t"),a("strong",[t._v("Our")]),t._v(" "),a("strong",[t._v("Contributions")])]),t._v(" "),a("ul",[a("li",[t._v("我们提出了MTFL算法，它在一般迭代FL算法的基础上增加了多任务学习，允许用户学习针对自己数据个性化的DNN模型。MTFL使用私人批量归一化（BN）层来实现这种归一化，这提供了额外的隐私优势。")]),t._v(" "),a("li",[t._v("提出了一个衡量FL算法性能的新指标：用户模型精度（UA）")]),t._v(" "),a("li",[t._v("我们分析了私有BN层在推理过程中对MTFL模型激活的影响，深入了解了其影响的来源")]),t._v(" "),a("li",[t._v("我们在MNIST和CI-FAR10数据集上进行了广泛的模拟")])]),t._v(" "),a("h3",{attrs:{id:"related-work"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#related-work"}},[t._v("#")]),t._v(" Related Work")]),t._v(" "),a("h4",{attrs:{id:"personalised-federated-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#personalised-federated-learning"}},[t._v("#")]),t._v(" Personalised Federated Learning")]),t._v(" "),a("p",[t._v("Meta- Learning元学习的目的是吧训练一个模型，该模型很容易用很少的样本进行微调。Others..🐳")]),t._v(" "),a("h4",{attrs:{id:"federated-learning-in-edge-computing"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#federated-learning-in-edge-computing"}},[t._v("#")]),t._v(" Federated Learning in Edge Computing")]),t._v(" "),a("p",[t._v("FL在网络边缘执行分布式计算，一些作者考虑了在这种环境下的FL的系统设计和通信成本。如通过选择具有最大梯度幅度的模型权重来减少客户端上传的总数据。")]),t._v(" "),a("h4",{attrs:{id:"federated-learning-performance"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#federated-learning-performance"}},[t._v("#")]),t._v(" Federated Learning Performance")]),t._v(" "),a("p",[t._v("FedAvg算法通过向参与的客户发送初始模型来协作训练模型，每个客户使用其本地数据在模型上形成SGD。将这些新模型发送到服务器进行平均，并开始新一轮。[29]提出在客户之间共享少量数据，以减少其数据分布的差异，并提高全局模型精度。")]),t._v(" "),a("p",[t._v("本文介绍的FedAvg-Adam优化方法在客户机上使用适应性优化，而不是SGD")]),t._v(" "),a("h3",{attrs:{id:"mtfl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mtfl"}},[t._v("#")]),t._v(" MTFL")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102031911.png",alt:"image-20220810203133817"}})]),t._v(" "),a("p",[t._v("MTFL算法在边缘计算环境中的应用")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("step1")]),t._v(" 服务器从其数据库中选择一部分客户机参与该轮🦌，并向他们发送工作请求")]),t._v(" "),a("li",[a("strong",[t._v("step2")]),t._v(" 客户机根据物理状态和本地首选项回复工作请求🐶")]),t._v(" "),a("li",[a("strong",[t._v("step3")]),t._v(" 客户机从服务器下载全局模型和优化参数，并使用专用补丁更新其全局模型副本，在本文中，使用BN层作为补丁🦐")]),t._v(" "),a("li",[a("strong",[t._v("step4")]),t._v(" 客户端进行本地训练，然后为下一轮保存个人补丁🐯")]),t._v(" "),a("li",[a("strong",[t._v("step5")]),t._v(" 服务器等待客户上传他们的非私有模型和优化值，或者直到时间限制🐍")]),t._v(" "),a("li",[a("strong",[t._v("step6")]),t._v(" 服务器平均所有模型，保存聚合，并开始新一轮")])]),t._v(" "),a("p",[t._v("因此，MTFL将绝大多数计算转移到执行实际模型训练的客户端设备。不仅用户数据未上传，而且其本地模型的关键部分也未上传。此外，MTFL利用补丁层来提高单个用户非IID数据集上的本地模型性能，更加个性化。")]),t._v(" "),a("h4",{attrs:{id:"user-model-accuracy-and-mtfl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#user-model-accuracy-and-mtfl"}},[t._v("#")]),t._v(" User Model Accuracy and MTFL")]),t._v(" "),a("p",[t._v("我们建议使用平均用户模型精度（UA）作为FL性能的替代指标，UA是使用本地测试集的客户端的精度。")]),t._v(" "),a("p",[t._v("在FL中，用户数据通常是非IID的，因此可以将用户视为具有不同但相关的学习任务🥚。")]),t._v(" "),a("p",[t._v("在FL中，目标是最小化一下目标函数🪵：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102056883.png",alt:"image-20220810205629832"}})]),t._v(" "),a("p",[t._v("其中，K是客户端总数，nk是客户端上的样本数，n是所有客户端上的样本总数，lk是客户端k上的损失函数，O是一组全局模型参数。")]),t._v(" "),a("p",[t._v("将独特的客户端补丁添加到FL中，形成的MTFL目标函数是💦：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102101406.png",alt:"image-20220810210136366"}})]),t._v(" "),a("p",[t._v("Mk是客户端上的补丁模型，由提供的模型参数O1，Oj组成，j表示联邦层的总数，Pk代表补丁参数。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102106329.png",alt:"image-20220810210625281"}})]),t._v(" "),a("p",[t._v("MTFL中使用的DNN模型的示例组成，每个客户机的模型都由共享的全局参数组成（O），用于卷积（Convoluional-Conv）和全连阶层(FC,Fullu-Connected)，以及私有批量归一化修补层，即Pk。")]),t._v(" "),a("p",[t._v("MTFL是将MTL合并到FL中的通用算法，可以使用不同的优化策略，我们随后表明，无论使用何种优化策略，MTFL都可以显著减少达到目标UA的轮数。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102113779.png",alt:"image-20220810211351719"}})]),t._v(" "),a("p",[t._v("在循环结束的时候，服务器根据GlobalModelUpdate, GlobalOptimUpdate创建新的globalmodel和optimiser.这些函数的增益取决于所使用的FL优化策略，在3.3节🪢。例如，FedAvg使用客户端模型的加权平均值进行全局模型更新。更新后的全局模型标志着本轮训练结束，新一轮训练开始。")]),t._v(" "),a("p",[t._v("[14]表明，BN层可以在集中设置中充当MTL的模型块，考虑到BN层在参数数量上非常轻量级，我们随后展示了BN层和MTFL中的补丁一样工作良好。BN层的定义：")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102123016.png",alt:"image-20220810212339961"}})]),t._v(" "),a("p",[t._v("。。。。。")]),t._v(" "),a("p",[t._v("我们选择使用BN层进行MTFL的个性化设置，一是因为它显示出卓越的个性化性能，二是BN层的存储成本非常小💰")]),t._v(" "),a("h4",{attrs:{id:"effect-of-bn-patches-on-inference"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#effect-of-bn-patches-on-inference"}},[t._v("#")]),t._v(" Effect of BN Patches on Inference")]),t._v(" "),a("p",[t._v("为了了解BN-patch层对UA的影响，我们考虑了在标记分级步骤之前和之后，客户端本地测试集内部DNN激活的变化。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102128024.png",alt:"image-20220810212844965"}})]),t._v(" "),a("p",[t._v("图3(a)显示，UA通常在迭代FL中的聚合步骤后下降📉")]),t._v(" "),a("p",[t._v("(a) lnd是独立训练，是平滑的精度曲线")]),t._v(" "),a("p",[t._v("(b) BN补丁层有助于将神经元i输出分布更靠近预聚集分布")]),t._v(" "),a("h4",{attrs:{id:"federated-optimisation-within-mtfl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#federated-optimisation-within-mtfl"}},[t._v("#")]),t._v(" Federated Optimisation within MTFL")]),t._v(" "),a("p",[t._v("如算法1所示，MTFL为每个客户端应用私有补丁层，并在LocalUpdate期间沿联邦（非私有）层对其进行训练。在每轮结束时，服务器聚合从客户端上传的联邦层（以及使用的任何分布式优化值），使用GlobalModelUpdate函数生成新的全局模型。")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102138318.png",alt:"image-20220810213818276"}})]),t._v(" "),a("p",[t._v("表1详细说明了不同的FL训练算法，其特征在于这些函数的实现。在FedAvg中，LocalUpdate只是一个小批量SGD，而Glob alModelUpdate将生成新的全局模型，作为上传的客户端模型的加权（按本地样本数）平均值。FedAvg使用无自适应优化的SGD，因此variableVin算法1是空值的元组，而GlobalOptimupdate不执行任何函数。")]),t._v(" "),a("p",[t._v("我们建议使用自适应优化（即Adam）作为分布式优化策略。我们称这种策略为FedAvg Adam。")]),t._v(" "),a("h3",{attrs:{id:"experiments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#experiments"}},[t._v("#")]),t._v(" Experiments")]),t._v(" "),a("h4",{attrs:{id:"datasets-and-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#datasets-and-models"}},[t._v("#")]),t._v(" Datasets and Models")]),t._v(" "),a("p",[t._v("使用两个图像分类数据集进行实验：MNIST和CIFAR10，以及两个DNN架构")]),t._v(" "),a("p",[t._v("MNIST：（28×28）像素灰度图像，由10类手写数字组成。该数据集上使用的“2NN”网络有一个由200个神经元组成的完全连接（FC）层，一个BN层，一个200神经元FC层和softmax输出层。")]),t._v(" "),a("p",[t._v("CIFAR10：来自10个类的对象的：（32×32）像素RGB图像。该数据集上使用的“CNN”网络有一个（3×3）卷积（conv）层，带有32个过滤器，然后是BN、ReLU和（2×2）最大池；第二个（3×3）conv ReLU层，具有64个过滤器、BN、ReLU和（2×2）最")]),t._v(" "),a("p",[t._v("大池；a512神经元ReLU-FC层；以及softmax输出层")]),t._v(" "),a("p",[t._v("在非IID客户上使用不同数量的客户软件、客户参与率和优化策略进行实验。")]),t._v(" "),a("h4",{attrs:{id:"patch-layers-in-fl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#patch-layers-in-fl"}},[t._v("#")]),t._v(" Patch Layers in FL")]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102149121.png",alt:"image-20220810214940062"}})]),t._v(" "),a("p",[a("img",{attrs:{src:"https://sjscs.oss-cn-hangzhou.aliyuncs.com/test/202208102149136.png",alt:"image-20220810214950098"}})])])}),[],!1,null,null,null);a.default=v.exports}}]);